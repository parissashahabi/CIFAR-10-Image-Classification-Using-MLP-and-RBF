{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30e5368",
   "metadata": {
    "id": "d30e5368"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f56ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017f587f",
   "metadata": {
    "id": "017f587f"
   },
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([    transforms.ToTensor(), transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9580db19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9580db19",
    "outputId": "a20385fb-fb10-492b-d966-63702b2a6f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading the train data\n",
    "batch_size = 100\n",
    "#drop_last=True\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size,shuffle=True )\n",
    "\n",
    "#loading the test data\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True )\n",
    "# You should define x_train and y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a09947",
   "metadata": {
    "id": "15a09947"
   },
   "source": [
    "### Dense (fully connected) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ef2947",
   "metadata": {
    "id": "d3ef2947"
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # He Weight Initialization\n",
    "        self.weights = torch.randn(n_inputs, n_neurons) * torch.sqrt(torch.tensor(2. / n_inputs))\n",
    "        self.biases = torch.zeros((1, n_neurons))\n",
    "        self.weights = self.weights.to(device)\n",
    "        self.biases = self.biases.to(device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return torch.matmul(self.inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        # calculating errors\n",
    "        self.inputs_error = torch.matmul(output_error, self.weights.T)\n",
    "        self.weights_grad = torch.matmul(self.inputs.T, output_error)\n",
    "        self.biases_grad = torch.sum(output_error, axis=0, keepdims=True)\n",
    "        return self.inputs_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1c92a",
   "metadata": {
    "id": "8de1c92a"
   },
   "source": [
    "### Activation Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7116c715",
   "metadata": {
    "id": "7116c715"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs.clone()\n",
    "        return torch.maximum(torch.zeros_like(inputs), inputs)\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        self.inputs_error = output_error.clone()\n",
    "        self.inputs_error[self.inputs <= 0] = 0\n",
    "        return self.inputs_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393a2d31",
   "metadata": {
    "id": "393a2d31"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,inputs):\n",
    "        self.outputs = 1 / (1 + np.exp(-inputs))\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self,output_error):\n",
    "        self.outputs_grad = output_error * (1-self.outputs) * self.outputs \n",
    "        return self.outputs_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ea7b84",
   "metadata": {
    "id": "e4ea7b84"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs.clone()\n",
    "        exp_inputs = torch.exp(self.inputs - torch.max(self.inputs, dim=1, keepdim=True).values)\n",
    "        self.outputs = exp_inputs / torch.sum(exp_inputs, dim=1, keepdim=True)\n",
    "        return self.outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78183ba",
   "metadata": {
    "id": "a78183ba"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e21c202",
   "metadata": {
    "id": "1e21c202"
   },
   "outputs": [],
   "source": [
    "class Categorical_Cross_Entropy_loss:\n",
    "    \n",
    "    def forward(self, softmax_output, class_label):\n",
    "        # Apply softmax function to the output\n",
    "        self.softmax_output = torch.clamp(softmax_output, 1e-12, 1. - 1e-12)\n",
    "        N = softmax_output.shape[0]\n",
    "        # Convert the class label to one-hot encoding\n",
    "        self.class_label = torch.zeros_like(softmax_output)\n",
    "        self.class_label[torch.arange(N), class_label] = 1\n",
    "        # Calculate the cross-entropy loss\n",
    "        self.loss = -torch.sum(self.class_label * torch.log(self.softmax_output)) / N\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, class_label):\n",
    "        # Calculate the derivative of the loss with respect to the softmax output\n",
    "        N = self.softmax_output.shape[0]\n",
    "        self.d_inputs = self.softmax_output.clone()\n",
    "        self.d_inputs[torch.arange(N), class_label] -= 1\n",
    "        self.d_inputs = self.d_inputs / N\n",
    "        return self.d_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2U7Kv8DRCpR",
   "metadata": {
    "id": "Q2U7Kv8DRCpR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6170d6",
   "metadata": {
    "id": "7b6170d6"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a109e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorScheduler:\n",
    "    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.01):\n",
    "        self.factor = factor\n",
    "        self.stop_factor_lr = stop_factor_lr\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "    def __call__(self, num_update):\n",
    "        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\n",
    "        return self.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af57ccac",
   "metadata": {
    "id": "af57ccac"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=learning_rate)\n",
    "    \n",
    "    def __call__(self, layer, num_epoch):\n",
    "        device = layer.weights.device\n",
    "        layer.weights = layer.weights.to(device)\n",
    "        layer.biases = layer.biases.to(device)\n",
    "        layer.weights -= self.scheduler(num_epoch) * layer.weights_grad.to(device)\n",
    "        layer.biases -= self.scheduler(num_epoch) * layer.biases_grad.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fv0oY86tsHER",
   "metadata": {
    "id": "Fv0oY86tsHER"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea3ce45",
   "metadata": {
    "id": "7ea3ce45"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4-p7NtSzPiXc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-p7NtSzPiXc",
    "outputId": "85dedb6d-1624-4fb0-e1e1-28b27c505036"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rrast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rrast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = resnet34(pretrained=True)\n",
    "num_features = feature_extractor.fc.in_features\n",
    "\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor.fc = nn.Identity()\n",
    "feature_extractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e940679d",
   "metadata": {
    "id": "e940679d"
   },
   "outputs": [],
   "source": [
    "#model\n",
    "Layer1 = Dense(num_features,20)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(20,10)\n",
    "Act2 = Softmax()\n",
    "Loss = Categorical_Cross_Entropy_loss()\n",
    "Optimizer = SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b9a7e",
   "metadata": {
    "id": "bb4b9a7e"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e1afab",
   "metadata": {
    "id": "21e1afab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.73it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 1.4843126\n",
      "Accuracy: 0.4802200\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 38.25it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Loss: 1.4841807\n",
      "Accuracy: 0.4837600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.60it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Loss: 1.4787741\n",
      "Accuracy: 0.4836400\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.80it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Loss: 1.4815349\n",
      "Accuracy: 0.4824000\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.84it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Loss: 1.4781501\n",
      "Accuracy: 0.4807800\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.17it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Loss: 1.4756937\n",
      "Accuracy: 0.4822200\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.81it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Loss: 1.4730776\n",
      "Accuracy: 0.4851600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.39it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Loss: 1.4694630\n",
      "Accuracy: 0.4869600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.60it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Loss: 1.4715542\n",
      "Accuracy: 0.4869600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.80it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Loss: 1.4744176\n",
      "Accuracy: 0.4833200\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:15<00:00, 31.43it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Loss: 1.4664201\n",
      "Accuracy: 0.4867200\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:14<00:00, 33.54it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Loss: 1.4683230\n",
      "Accuracy: 0.4859200\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.06it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Loss: 1.4670257\n",
      "Accuracy: 0.4851800\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.86it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Loss: 1.4643761\n",
      "Accuracy: 0.4912000\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.42it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Loss: 1.4616177\n",
      "Accuracy: 0.4915000\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.05it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "Loss: 1.4631858\n",
      "Accuracy: 0.4884800\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 37.72it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "Loss: 1.4546152\n",
      "Accuracy: 0.4911600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 38.76it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18\n",
      "Loss: 1.4594784\n",
      "Accuracy: 0.4895600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:12<00:00, 39.41it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19\n",
      "Loss: 1.4539921\n",
      "Accuracy: 0.4950000\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [00:13<00:00, 36.44it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n",
      "Loss: 1.4597016\n",
      "Accuracy: 0.4905600\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for x_train, y_train in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", colour=\"blue\"):\n",
    "        # Forward pass\n",
    "        x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "        x = feature_extractor(x_train)\n",
    "        x = Layer1.forward(x)\n",
    "        x = Act1.forward(x)\n",
    "        x = Layer2.forward(x)\n",
    "        x = Act2.forward(x)\n",
    "        loss = Loss.forward(x, y_train)\n",
    "\n",
    "        # Report batch metrics\n",
    "        y_predict = torch.argmax(x, dim=1)\n",
    "        accuracy = torch.mean((y_train == y_predict).float())\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_accuracy += accuracy.item()\n",
    "\n",
    "        # Backward pass\n",
    "        x = Loss.backward(y_train)\n",
    "        #x = Act2.backward(x)\n",
    "        x = Layer2.backward(x)\n",
    "        x = Act1.backward(x)\n",
    "        x = Layer1.backward(x)\n",
    "\n",
    "        # Update parameters\n",
    "        Optimizer(Layer1, epoch)\n",
    "        Optimizer(Layer2, epoch)\n",
    "\n",
    "    # Report epoch metrics\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    epoch_accuracy /= len(train_dataloader)\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'Loss: {epoch_loss:.7f}')\n",
    "    print(f'Accuracy: {epoch_accuracy:.7f}')\n",
    "    print('--------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5892666",
   "metadata": {
    "id": "c5892666"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e0237a",
   "metadata": {
    "id": "f7e0237a"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Confusion Matrix for the training set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cm_train \u001b[38;5;241m=\u001b[39m \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_predict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      4\u001b[0m sb\u001b[38;5;241m.\u001b[39mheatmap(cm_train, annot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:317\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[0;32m    233\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    234\u001b[0m ):\n\u001b[0;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:87\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m---> 87\u001b[0m type_true \u001b[38;5;241m=\u001b[39m \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m y_type \u001b[38;5;241m=\u001b[39m {type_true, type_pred}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:309\u001b[0m, in \u001b[0;36mtype_of_target\u001b[1;34m(y, input_name)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be class \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseSeries\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSparseArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\multiclass.py:169\u001b[0m, in \u001b[0;36mis_multilabel\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    167\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_kwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#Confusion Matrix for the training set\n",
    "cm_train = confusion_matrix(y_train, y_predict)\n",
    "plt.subplots(figsize=(10, 6))\n",
    "sb.heatmap(cm_train, annot = True, fmt = 'g')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for the training set\")\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix for the test set\n",
    "# // To Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311dd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf095b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2c386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7b2c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070d8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
