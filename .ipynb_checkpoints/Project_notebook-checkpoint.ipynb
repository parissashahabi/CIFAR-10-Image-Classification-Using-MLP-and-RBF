{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30e5368",
   "metadata": {
    "id": "d30e5368"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet34\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017f587f",
   "metadata": {
    "id": "017f587f"
   },
   "outputs": [],
   "source": [
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([    transforms.ToTensor(), transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9580db19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9580db19",
    "outputId": "a20385fb-fb10-492b-d966-63702b2a6f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 170498071/170498071 [00:45<00:00, 3757048.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# loading the train data\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size=100,shuffle=True)\n",
    "\n",
    "#loading the test data\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100, shuffle=True)\n",
    "# You should define x_train and y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a09947",
   "metadata": {
    "id": "15a09947"
   },
   "source": [
    "### Dense (fully connected) layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ef2947",
   "metadata": {
    "id": "d3ef2947"
   },
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # He Weight Initialization\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2 / n_inputs)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self,output_error):\n",
    "        # calculating errors\n",
    "        self.inputs_error = np.dot(output_error, self.weights.T)\n",
    "        self.weights_grad = np.dot(self.inputs.T, output_error)\n",
    "        self.biases_grad = output_error\n",
    "        return self.inputs_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "YFY9oY_79sHt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFY9oY_79sHt",
    "outputId": "61c13895-4fb4-42db-aa85-da1af24cb3bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(1, 10)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w = np.random.randn(10, 10) * np.sqrt(2 / 10)\n",
    "b = np.zeros((1, 10))\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de1c92a",
   "metadata": {
    "id": "8de1c92a"
   },
   "source": [
    "### Activation Layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7116c715",
   "metadata": {
    "id": "7116c715"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.maximum(0,self.inputs)\n",
    "\n",
    "    def backward(self, output_error):\n",
    "        return output_error > 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393a2d31",
   "metadata": {
    "id": "393a2d31"
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def forward(self,inputs):\n",
    "        self.outputs = 1 / (1 + np.exp(-inputs))\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self,output_error):\n",
    "        self.outputs_grad = output_error * (1-self.outputs) * self.outputs \n",
    "        return self.outputs_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ea7b84",
   "metadata": {
    "id": "e4ea7b84"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_inputs = np.exp(inputs)\n",
    "        self.output = exp_inputs / np.sum(exp_inputs, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_error):\n",
    "        # For a softmax output y and target t, the derivative dy/dx_i can be computed as:\n",
    "        # dy/dx_i = y_i * (1 - y_i) if i=j, and -y_i * y_j if i!=j\n",
    "        # where j is the index of the correct class\n",
    "        # More details: https://deepnotes.io/softmax-crossentropy#derivative-of-softmax\n",
    "        \n",
    "        # Here, output_error is the derivative of the loss with respect to the output of the softmax layer.\n",
    "        # We need to compute the derivative of the loss with respect to the input to the softmax layer.\n",
    "        # Using the chain rule, this can be written as:\n",
    "        # dL/dx_i = dL/dy_j * dy_j/dx_i, where j is the index of the correct class\n",
    "        # dy_j/dx_i can be computed using the above formula\n",
    "        \n",
    "        self.input_error = output_error * self.output - output_error * np.sum(self.output * self.output, axis=1, keepdims=True)\n",
    "        return self.input_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78183ba",
   "metadata": {
    "id": "a78183ba"
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e21c202",
   "metadata": {
    "id": "1e21c202"
   },
   "outputs": [],
   "source": [
    "class Categorical_Cross_Entropy_loss:\n",
    "    def forward(self,softmax_output,class_label):\n",
    "        self.softmax_output = softmax_output\n",
    "        self.class_label = class_label\n",
    "        self.batch_size = softmax_output.shape[0]\n",
    "        self.loss = -np.sum(np.log(softmax_output[np.arange(self.batch_size), class_label])) / self.batch_size\n",
    "        return self.loss\n",
    "        \n",
    "    def backward(self,softmax_output,class_label):\n",
    "        # The derivative of the CCE loss with respect to the softmax output y_i can be computed as:\n",
    "        # dL/dy_i = y_i - t_i, where t_i is 1 if i is the index of the correct class, and 0 otherwise\n",
    "        # More details: https://deepnotes.io/softmax-crossentropy\n",
    "        \n",
    "        # Here, softmax_output is the output of the softmax layer, and class_label is the index of the correct class for each input\n",
    "        # We need to compute the derivative of the loss with respect to the input to the softmax layer.\n",
    "        # Using the chain rule, this can be written as:\n",
    "        # dL/dx_i = dL/dy_i * dy_i/dx_i, where dy_i/dx_i can be computed using the softmax backward function.\n",
    "        \n",
    "        output_error = softmax_output.copy()\n",
    "        output_error[np.arange(self.batch_size), class_label] -= 1\n",
    "        output_error /= self.batch_size\n",
    "        \n",
    "        return output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q2U7Kv8DRCpR",
   "metadata": {
    "id": "Q2U7Kv8DRCpR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6170d6",
   "metadata": {
    "id": "7b6170d6"
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ZP42FBMAMK",
   "metadata": {
    "id": "35ZP42FBMAMK"
   },
   "outputs": [],
   "source": [
    "class CosineScheduler:\n",
    "    def __init__(self, max_update, base_lr=0.01, final_lr=0,\n",
    "               warmup_steps=0, warmup_begin_lr=0):\n",
    "        self.base_lr_orig = base_lr\n",
    "        self.max_update = max_update\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_begin_lr = warmup_begin_lr\n",
    "        self.max_steps = self.max_update - self.warmup_steps\n",
    "\n",
    "    def get_warmup_lr(self, epoch):\n",
    "        increase = (self.base_lr_orig - self.warmup_begin_lr) \\\n",
    "                       * float(epoch) / float(self.warmup_steps)\n",
    "        return self.warmup_begin_lr + increase\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        if epoch < self.warmup_steps:\n",
    "            return self.get_warmup_lr(epoch)\n",
    "        if epoch <= self.max_update:\n",
    "            self.base_lr = self.final_lr + (\n",
    "                self.base_lr_orig - self.final_lr) * (1 + math.cos(\n",
    "                math.pi * (epoch - self.warmup_steps) / self.max_steps)) / 2\n",
    "        return self.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a109e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorScheduler:\n",
    "    def __init__(self, factor=1, stop_factor_lr=1e-7, base_lr=0.001):\n",
    "        self.factor = factor\n",
    "        self.stop_factor_lr = stop_factor_lr\n",
    "        self.base_lr = base_lr\n",
    "\n",
    "    def __call__(self, num_update):\n",
    "        self.base_lr = max(self.stop_factor_lr, self.base_lr * self.factor)\n",
    "        return self.base_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af57ccac",
   "metadata": {
    "id": "af57ccac"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.scheduler = FactorScheduler(factor=0.9, stop_factor_lr=1e-2, base_lr=2.0)\n",
    "    def __call__(self, layer, num_epoch):\n",
    "        # Update layer parameters based on gradient descent rule\n",
    "        layer.weights = layer.weights - self.scheduler(num_epoch) * layer.weights_grad\n",
    "        layer.biases  =  layer.biases - self.scheduler(num_epoch) * layer.biases_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fv0oY86tsHER",
   "metadata": {
    "id": "Fv0oY86tsHER"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ea3ce45",
   "metadata": {
    "id": "7ea3ce45"
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4-p7NtSzPiXc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-p7NtSzPiXc",
    "outputId": "85dedb6d-1624-4fb0-e1e1-28b27c505036"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rrast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\rrast\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = resnet34(pretrained=True)\n",
    "num_features = feature_extractor.fc.in_features\n",
    "\n",
    "for param in feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "feature_extractor.fc = nn.Identity() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e940679d",
   "metadata": {
    "id": "e940679d"
   },
   "outputs": [],
   "source": [
    "#model\n",
    "Layer1 = Dense(num_features,20)\n",
    "Act1 = ReLU()\n",
    "Layer2 = Dense(20,10)\n",
    "Act2 = Softmax()\n",
    "Loss = Categorical_Cross_Entropy_loss()\n",
    "Optimizer = SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b9a7e",
   "metadata": {
    "id": "bb4b9a7e"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1afab",
   "metadata": {
    "id": "21e1afab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:57<00:00,  4.26it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Loss: 2.30608\n",
      "Accuracy: 0.10076\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:52<00:00,  4.45it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Loss: 2.30259\n",
      "Accuracy: 0.09920\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:51<00:00,  4.49it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10116\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:48<00:00,  4.60it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "Loss: 2.30260\n",
      "Accuracy: 0.09976\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:48<00:00,  4.61it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.09952\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:50<00:00,  4.54it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10054\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:47<00:00,  4.64it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "Loss: 2.30259\n",
      "Accuracy: 0.10040\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:50<00:00,  4.54it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10140\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|\u001b[34m███████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:44<00:00,  4.77it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Loss: 2.30260\n",
      "Accuracy: 0.10050\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:44<00:00,  4.79it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "Loss: 2.30259\n",
      "Accuracy: 0.09926\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:43<00:00,  4.84it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10102\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:37<00:00,  5.11it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "Loss: 2.30259\n",
      "Accuracy: 0.09936\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:31<00:00,  5.45it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.09970\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:30<00:00,  5.52it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10138\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:34<00:00,  5.31it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.09972\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|\u001b[34m██████████████████████████████████████████████████████████████████████\u001b[0m| 500/500 [01:42<00:00,  4.86it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16\n",
      "Loss: 2.30258\n",
      "Accuracy: 0.10074\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:  41%|\u001b[34m████████████████████████████▋                                         \u001b[0m| 205/500 [00:46<01:04,  4.56it/s]\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    i = 0 \n",
    "    for x_train, y_train in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", colour=\"blue\"):\n",
    "        # Forward pass\n",
    "        x = feature_extractor(x_train)\n",
    "        x = Layer1.forward(x)\n",
    "        x = Act1.forward(x)\n",
    "        x = Layer2.forward(x)\n",
    "        x = Act2.forward(x)\n",
    "        loss = Loss.forward(x, y_train)\n",
    "\n",
    "        # Report batch metrics\n",
    "        y_predict = np.argmax(x, axis=1)\n",
    "        accuracy = np.mean(y_train.numpy() == y_predict)\n",
    "        epoch_loss += loss\n",
    "        epoch_accuracy += accuracy\n",
    "\n",
    "        # Backward pass\n",
    "        x = Loss.backward(x, y_train)\n",
    "        x = Act2.backward(x)\n",
    "        x = Layer2.backward(x)\n",
    "        x = Act1.backward(x)\n",
    "        x = Layer1.backward(x)\n",
    "\n",
    "        # Update parameters\n",
    "        Optimizer(Layer1, epoch)\n",
    "        Optimizer(Layer2, epoch)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Report epoch metrics\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    epoch_accuracy /= len(train_dataloader)\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'Loss: {epoch_loss:.5f}')\n",
    "    print(f'Accuracy: {epoch_accuracy:.5f}')\n",
    "    print('--------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5892666",
   "metadata": {
    "id": "c5892666"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0237a",
   "metadata": {
    "id": "f7e0237a"
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix for the training set\n",
    "cm_train = confusion_matrix(y_train, y_predict)\n",
    "plt.subplots(figsize=(10, 6))\n",
    "sb.heatmap(cm_train, annot = True, fmt = 'g')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix for the training set\")\n",
    "plt.show()\n",
    "\n",
    "#Confusion Matrix for the test set\n",
    "# // To Do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba3c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311dd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf095b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2c386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
